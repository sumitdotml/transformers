{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do Masking\n",
    "\n",
    "2 types that are used in transformer:\n",
    "- Padding Mask: Ensures the model does not pay attention to the padding tokens added to sequences within a batch to make them the same length.\n",
    "- Causal Mask (or Look-Ahead Mask): Used specifically in decoder self-attention to prevent a position from attending to subsequent (future) positions. This maintains the autoregressive property â€“ predictions for position `i` can only depend on outputs at positions `<i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3607, -0.2859, -0.3938]),\n",
       " tensor([ 0.2429, -1.3833]),\n",
       " tensor([-2.3134, -0.3172, -0.8660,  1.7482, -0.2759]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "a = torch.randn(3)\n",
    "b = torch.randn(2)\n",
    "c = torch.randn(5)\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to determine the maximum seq_len in these batches (3) and then increase the seq_len of every other batch to the max seq_len using a padding token ID of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_len, pad_value=0.0):\n",
    "    # Convert sequence to tensor if it's not already\n",
    "    seq_tensor = torch.tensor(seq) if not isinstance(seq, torch.Tensor) else seq\n",
    "    current_len = len(seq_tensor)\n",
    "    \n",
    "    if current_len < max_len:\n",
    "        # Create padding\n",
    "        padding = torch.full((max_len - current_len,), pad_value)\n",
    "        # Concatenate the original sequence with padding\n",
    "        return torch.cat([seq_tensor, padding])\n",
    "    return seq_tensor[:max_len]  # Truncate if longer than max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lengths: 3 2 5\n",
      "After padding: 5 5 5\n",
      "\n",
      "Padded sequences:\n",
      "a: tensor([ 0.3607, -0.2859, -0.3938,  0.0000,  0.0000])\n",
      "b: tensor([ 0.2429, -1.3833,  0.0000,  0.0000,  0.0000])\n",
      "c: tensor([-2.3134, -0.3172, -0.8660,  1.7482, -0.2759])\n"
     ]
    }
   ],
   "source": [
    "# Find max length among sequences\n",
    "max_len = max(len(a), len(b), len(c))\n",
    "\n",
    "# Pad all sequences to max_len\n",
    "padded_a = pad_sequence(a, max_len)\n",
    "padded_b = pad_sequence(b, max_len)\n",
    "padded_c = pad_sequence(c, max_len)\n",
    "\n",
    "print('Original lengths:', len(a), len(b), len(c))\n",
    "print('After padding:', len(padded_a), len(padded_b), len(padded_c))\n",
    "print('\\nPadded sequences:')\n",
    "print('a:', padded_a)\n",
    "print('b:', padded_b)\n",
    "print('c:', padded_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding masks:\n",
      "a: tensor([[[1., 1., 1., 0., 0.]]])\n",
      "b: tensor([[[1., 1., 0., 0., 0.]]])\n",
      "c: tensor([[[1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(sequence):\n",
    "    # Create mask (1 for real values, 0 for padding)\n",
    "    return (sequence != 0).float().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Example usage\n",
    "mask_a = create_padding_mask(padded_a)\n",
    "mask_b = create_padding_mask(padded_b)\n",
    "mask_c = create_padding_mask(padded_c)\n",
    "\n",
    "print('Padding masks:')\n",
    "print('a:', mask_a)\n",
    "print('b:', mask_b)\n",
    "print('c:', mask_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tensors = [a, b, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.3607, -0.2859, -0.3938]),\n",
       " tensor([ 0.2429, -1.3833]),\n",
       " tensor([-2.3134, -0.3172, -0.8660,  1.7482, -0.2759])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3607, -0.2859, -0.3938,  0.0000,  0.0000],\n",
       "        [ 0.2429, -1.3833,  0.0000,  0.0000,  0.0000],\n",
       "        [-2.3134, -0.3172, -0.8660,  1.7482, -0.2759]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "padded_batch = pad_sequence(sequences=list_of_tensors, batch_first=True, padding_value=0.0)\n",
    "padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask_bool = (padded_batch != 0)\n",
    "padding_mask = padding_mask_bool.float()\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the mask for broadcasting: [batch_size, 1, 1, key_seq_len]\n",
    "# This shape is needed so it aligns with attn_scores [batch, num_heads, query_seq_len, key_seq_len]\n",
    "# The mask will be applied based on the KEY sequence.\n",
    "padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a proper function in a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs Batch Shape: torch.Size([3, 7])\n",
      "Padding Mask Shape: torch.Size([3, 1, 1, 7])\n",
      "\n",
      "Padding Mask (Batch 0):\n",
      "tensor([1., 1., 1., 1., 0., 0., 0.])\n",
      "\n",
      "Padding Mask (Batch 1):\n",
      "tensor([1., 1., 1., 0., 0., 0., 0.])\n",
      "\n",
      "Padding Mask (Batch 2):\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(input_ids: torch.Tensor, padding_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a padding mask for multi-head attention based on input token IDs.\n",
    "\n",
    "    The mask identifies positions in the sequence that correspond to padding tokens.\n",
    "    It's shaped for broadcasting compatibility with attention scores inside MHA.\n",
    "\n",
    "    Convention:\n",
    "        - 1.0 : Represents a token that should be attended to (kept).\n",
    "        - 0.0 : Represents a padding token that should be masked out (ignored).\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): A tensor of token IDs with shape\n",
    "            [batch_size, sequence_length].\n",
    "        padding_idx (int, optional): The index representing the padding token\n",
    "            in the vocabulary. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A float tensor representing the padding mask with shape\n",
    "            [batch_size, 1, 1, sequence_length]. Ready to be used in\n",
    "            MultiHeadAttention.\n",
    "    \"\"\"\n",
    "    # 1. Create boolean mask: True where input is NOT padding, False where it IS padding.\n",
    "    # Shape: [batch_size, sequence_length]\n",
    "    mask = (input_ids != padding_idx)\n",
    "\n",
    "    # 2. Convert boolean mask to float (True -> 1.0, False -> 0.0).\n",
    "    # This matches the convention needed for `masked_fill(mask == 0, -inf)`.\n",
    "    # Shape: [batch_size, sequence_length]\n",
    "    mask = mask.float()\n",
    "\n",
    "    # 3. Reshape for broadcasting within MHA.\n",
    "    # Add dimensions for `num_heads` (dim 1) and `query_sequence_length` (dim 2).\n",
    "    # The mask applies based on the key sequence length (the last dimension).\n",
    "    # Shape: [batch_size, 1, 1, sequence_length]\n",
    "    # The mask tensor will automatically be on the same device as input_ids.\n",
    "    return mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Assume we have a batch of token IDs after tokenization and padding\n",
    "# (batch_size=3, seq_len=7, padding_idx=0)\n",
    "input_ids_batch = torch.tensor([\n",
    "    [101, 567, 890, 102,   0,   0,   0], # Seq length 4\n",
    "    [101, 432, 102,   0,   0,   0,   0], # Seq length 3\n",
    "    [101, 666, 777, 888, 999, 555, 102]  # Seq length 7 (no padding)\n",
    "])\n",
    "\n",
    "# Create the padding mask using the function\n",
    "padding_mask = create_padding_mask(input_ids_batch, padding_idx=0)\n",
    "\n",
    "print(\"Input IDs Batch Shape:\", input_ids_batch.shape)\n",
    "print(\"Padding Mask Shape:\", padding_mask.shape)\n",
    "print(\"\\nPadding Mask (Batch 0):\")\n",
    "# Print the mask for the first item in the batch (squeeze extra dims for readability)\n",
    "print(padding_mask[0].squeeze())\n",
    "print(\"\\nPadding Mask (Batch 1):\")\n",
    "print(padding_mask[1].squeeze())\n",
    "print(\"\\nPadding Mask (Batch 2):\")\n",
    "print(padding_mask[2].squeeze())\n",
    "\n",
    "# --- How to integrate with our MHA ---\n",
    "\n",
    "# Assume 'embeddings' is the output of our nn.Embedding layer applied to input_ids_batch\n",
    "# embeddings = embedding_layer(input_ids_batch) -> Shape [3, 7, 512]\n",
    "\n",
    "# Instantiate our MHA module\n",
    "# multihead_attn = MultiHeadAttention(num_heads=8, d_model=512, dropout=0.1)\n",
    "\n",
    "# Pass the embeddings and the created mask to the forward method\n",
    "# output, attn_weights = multihead_attn(\n",
    "#     query_input=embeddings,\n",
    "#     key_input=embeddings,   # For self-attention\n",
    "#     value_input=embeddings, # For self-attention\n",
    "#     mask=padding_mask       # Pass the generated mask here\n",
    "# )\n",
    "\n",
    "# print(\"\\nOutput shape from MHA:\", output.shape)\n",
    "# print(\"Attention weights shape:\", attn_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
