{
  "model_type": "transformer",
  "hidden_dim": 512,
  "vocab_size": 30000,
  "output_dim": 10,
  "num_encoder_layers": 6,
  "num_decoder_layers": 6,
  "num_attention_heads": 8,
  "ff_dim": 2048,
  "dropout": 0.1,
  "attention_dropout": 0.1,
  "activation_function": "gelu",
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "batch_size": 32,
  "learning_rate": 5e-5,
  "weight_decay": 0.01,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-8,
  "warmup_steps": 10000,
  "epochs": 10,
  "optimizer": "adamw"
} 