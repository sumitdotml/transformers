# LLaMA3

It turns out that the LLaMA3 architecture is almost identical to that of LLaMA2. The only difference is the use of a new tokenizer called Tiktoken. lol

Idk whether I'll rewrite this whole architecture code again. Maybe I'll focus more on the inference part using my LlaMA2 implementation code and use it to load the LLaMA3 weights on Hugging Face. But I'll do that later. If you want to see the architecture code, you can check out [LLaMA2](../llama2) (these two are the same).

## Resources

- [Building LlaMA3 from scratch by Lightning AI](https://lightning.ai/fareedhassankhan12/studios/building-llama-3-from-scratch?section=featured)
- [Decoding LLaMA3: An explainer for tinkerers](https://hasgeek.com/simrathanspal/the-llama3-guide/sub/decoding-llama3-part-1-intro-to-llama3-RCehJkfUH348ryim1x6PLN)
- [Deep dive into LlaMA3 by hand](http://medium.com/data-science/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2)

