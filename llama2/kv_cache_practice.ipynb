{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Concrete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_k after filling Head 0 with torch.arange(0, 4):\n",
      "tensor([[[[0., 1., 2., 3.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "cache_k after filling Head 0 with torch.arange(4, 8):\n",
      "tensor([[[[0., 1., 2., 3.],\n",
      "          [4., 5., 6., 7.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n",
      "cache_k after filling Head 0 with torch.arange(8, 12):\n",
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.]]]])\n",
      "cache_k after filling Head 1 with torch.arange(100, 104):\n",
      "tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [  0.,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,   0.]]]])\n",
      "cache_k after filling Head 1 with torch.arange(104, 108):\n",
      "tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [  0.,   0.,   0.,   0.]]]])\n",
      "cache_k after filling Head 1 with torch.arange(108, 112):\n",
      "tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [108., 109., 110., 111.]]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 1\n",
    "num_kv_heads = 2\n",
    "seq_len_cache = 3\n",
    "seq_len_curr = 1\n",
    "head_dim = 4\n",
    "\n",
    "# --- 1. Simulate the Cache (Past K values) ---\n",
    "# We have K vectors for 3 past tokens (p=0, 1, 2) for 2 KV heads.\n",
    "# Shape: [1, 2, 3, 4]\n",
    "# Let's fill it with distinct values to see where they go.\n",
    "# Head 0: values 0-11 represent features for tokens 0, 1, 2\n",
    "# Head 1: values 100-111 represent features for tokens 0, 1, 2\n",
    "cache_k = torch.zeros(batch_size, num_kv_heads, seq_len_cache, head_dim)\n",
    "# Fill Head 0\n",
    "cache_k[0, 0, 0, :] = torch.arange(0, 4)    # Token 0, Head 0 Features\n",
    "print(f\"cache_k after filling Head 0 with torch.arange(0, 4):\\n{cache_k}\")\n",
    "cache_k[0, 0, 1, :] = torch.arange(4, 8)    # Token 1, Head 0 Features\n",
    "print(f\"cache_k after filling Head 0 with torch.arange(4, 8):\\n{cache_k}\")\n",
    "cache_k[0, 0, 2, :] = torch.arange(8, 12)   # Token 2, Head 0 Features\n",
    "print(f\"cache_k after filling Head 0 with torch.arange(8, 12):\\n{cache_k}\")\n",
    "# Fill Head 1\n",
    "cache_k[0, 1, 0, :] = torch.arange(100, 104)  # Token 0, Head 1 Features\n",
    "print(f\"cache_k after filling Head 1 with torch.arange(100, 104):\\n{cache_k}\")\n",
    "cache_k[0, 1, 1, :] = torch.arange(104, 108)  # Token 1, Head 1 Features\n",
    "print(f\"cache_k after filling Head 1 with torch.arange(104, 108):\\n{cache_k}\")\n",
    "cache_k[0, 1, 2, :] = torch.arange(108, 112) # Token 2, Head 1 Features\n",
    "print(f\"cache_k after filling Head 1 with torch.arange(108, 112):\\n{cache_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cache K ---\n",
      "Shape: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [108., 109., 110., 111.]]]])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Cache K ---\")\n",
    "print(f\"Shape: {cache_k.shape}\")\n",
    "print(cache_k)\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_k after filling Head 0 with torch.arange(50, 54):\n",
      "tensor([[[[50., 51., 52., 53.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.,  0.]]]])\n",
      "current_k after filling Head 1 with torch.arange(150, 154):\n",
      "tensor([[[[ 50.,  51.,  52.,  53.]],\n",
      "\n",
      "         [[150., 151., 152., 153.]]]])\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Simulate the Current K value ---\n",
    "# We calculate the K vector for the *new* token (at position p=3).\n",
    "# Shape: [1, 2, 1, 4]\n",
    "# Head 0: values 50-53 represent features for token 3\n",
    "# Head 1: values 150-153 represent features for token 3\n",
    "current_k = torch.zeros(batch_size, num_kv_heads, seq_len_curr, head_dim)\n",
    "# Fill Head 0\n",
    "current_k[0, 0, 0, :] = torch.arange(50, 54) # Token 3, Head 0 Features\n",
    "print(f\"current_k after filling Head 0 with torch.arange(50, 54):\\n{current_k}\")\n",
    "# Fill Head 1\n",
    "current_k[0, 1, 0, :] = torch.arange(150, 154) # Token 3, Head 1 Features\n",
    "print(f\"current_k after filling Head 1 with torch.arange(150, 154):\\n{current_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Current K ---\n",
      "Shape: torch.Size([1, 2, 1, 4])\n",
      "tensor([[[[ 50.,  51.,  52.,  53.]],\n",
      "\n",
      "         [[150., 151., 152., 153.]]]])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Current K ---\")\n",
    "print(f\"Shape: {current_k.shape}\")\n",
    "print(current_k)\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- K Total (After Concatenation) ---\n",
      "Shape: torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[  0.,   1.,   2.,   3.],\n",
      "          [  4.,   5.,   6.,   7.],\n",
      "          [  8.,   9.,  10.,  11.],\n",
      "          [ 50.,  51.,  52.,  53.]],\n",
      "\n",
      "         [[100., 101., 102., 103.],\n",
      "          [104., 105., 106., 107.],\n",
      "          [108., 109., 110., 111.],\n",
      "          [150., 151., 152., 153.]]]])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Perform Concatenation along dim=2 (Sequence Length) ---\n",
    "k_total = torch.cat([cache_k, current_k], dim=2)\n",
    "\n",
    "print(\"--- K Total (After Concatenation) ---\")\n",
    "print(f\"Shape: {k_total.shape}\")\n",
    "print(k_total)\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, what happened here?\n",
    "\n",
    "```python\n",
    "# Cache K\n",
    "torch.Size([1, 2, 3, 4])\n",
    "\n",
    "tensor([[[[ 0., 1., 2., 3.], # head 0, token 0, 1, 2\n",
    "          [ 4., 5., 6., 7.],\n",
    "          [ 8., 9., 10., 11.]],\n",
    "     \n",
    "        [[100., 101., 102., 103.], # head 1, token 0, 1, 2\n",
    "        [104., 105., 106., 107.],\n",
    "        [108., 109., 110., 111.]]]])\n",
    "\n",
    "# Current K\n",
    "torch.Size([1, 2, 1, 4])\n",
    "\n",
    "tensor([[[[ 50., 51., 52., 53.]], # head 0, token 3\n",
    "     [[150., 151., 152., 153.]]]]) # head 1, token 3\n",
    "\n",
    "# --- K Total (After Concatenation)\n",
    "torch.Size([1, 2, 4, 4])\n",
    "\n",
    "tensor([[[[ 0., 1., 2., 3.], # head 0, token 0, 1, 2, 3\n",
    "          [ 4., 5., 6., 7.],\n",
    "          [ 8., 9., 10., 11.],\n",
    "          [ 50., 51., 52., 53.]],\n",
    "\n",
    "          [[100., 101., 102., 103.], # head 1, token 0, 1, 2, 3  \n",
    "          [104., 105., 106., 107.],\n",
    "          [108., 109., 110., 111.],\n",
    "          [150., 151., 152., 153.]]]])\n",
    "```\n",
    "\n",
    "In other words, when we did concatenation alongside the `seq_len` dim and thus increased it from 3 to 4, we just added the last slice of `head_dim` to the respective head! So the `head_dim=4` is still the same (since the 4th 'row' has same number of elements in it), but since we added the 4th 'row', it increased the `seq_len`.\n",
    "\n",
    "Important bits:\n",
    "- What was added: the data corresponding to the new token (position `p=3`). This data wasn't just a \"slice of `head_dim`\"; it was a complete slice along the sequence dimension (dim=2) with shape `[1, 2, 1, 4]`. This slice contains the full `head_dim=4` feature vector for each of the `num_kv_heads=2`.\n",
    "- Preservation of `head_dim`: the `head_dim` (the last dimension, size 4) remained unchanged. The vectors representing the rich features for each token (e.g., [0., 1., 2., 3.] for token 0/head 0, or [50., 51., 52., 53.] for token 3/head 0) were kept intact.\n",
    "- Increase in `seq_len`: The only dimension that changed was `dim=2` (the sequence length dimension), increasing from 3 (cache) to 4 (total) because we effectively added one position's worth of data to the sequence history.\n",
    "\n",
    "I asked Gemini 2.5 whether this understanding is correct, and this was the response:\n",
    "\n",
    "> Your refined understanding is spot on. Concatenating along `dim=2` acts like appending the complete Key/Value representation (spanning all heads and the full `head_dim` feature vector) of the new token(s) to the end of the list of representations for the previously processed tokens. It extends the timeline (`seq_len`) while preserving the richness (`head_dim`) of the information stored for each point in time. \n",
    "> You've nailed the key insight.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
